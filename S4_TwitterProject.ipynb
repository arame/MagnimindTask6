{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2023-02-05 22:13:08,095 | INFO : Cuda is available: True\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd \n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.optim import lr_scheduler\n",
    "import torchvision\n",
    "from torchvision import datasets, models, transforms\n",
    "from torchvision.models import ResNet18_Weights\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "import matplotlib.pyplot as plt\n",
    "from PIL import Image\n",
    "from pathlib import Path\n",
    "import logging, os, glob\n",
    "from _logging import set_logging\n",
    "from _metrics import display_metrics\n",
    "from _pckle import save_pickle_object, load_pickle_object\n",
    "from _utility import gl, get_perc, get_dictionaries_from_list\n",
    "from _model import train_model\n",
    "\n",
    "set_logging(logging)\n",
    "logging.info(f\"Cuda is available: {torch.cuda.is_available()}\")\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "classes = [\"Business\", \"Other\"]\n",
    "dict_classes, dict_classes_rev = get_dictionaries_from_list(classes)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2023-02-05 22:13:08,191 | INFO : Loading pickle file from: pickle\\pkle_train_loader.pkl\n",
      "2023-02-05 22:13:08,828 | INFO : Loading pickle file from: pickle\\pkle_val_loader.pkl\n",
      "2023-02-05 22:13:08,884 | INFO : Loading pickle file from: pickle\\pkle_test_loader.pkl\n"
     ]
    }
   ],
   "source": [
    "train_loader = load_pickle_object(gl.pkl_train_loader)\n",
    "val_loader = load_pickle_object(gl.pkl_val_loader)\n",
    "test_loader = load_pickle_object(gl.pkl_test_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def imshow(inp, _mean, _std, title=None):\n",
    "    \"\"\"Imshow for Tensor.\"\"\"\n",
    "    inp = inp.numpy().transpose((1, 2, 0))\n",
    "    mean = np.array(_mean)\n",
    "    std = np.array(_std)\n",
    "    inp = std * inp + mean  # denormalise\n",
    "    inp = np.clip(inp, 0, 1)\n",
    "    plt.imshow(inp)\n",
    "    if title is not None:\n",
    "        plt.title(title)\n",
    "    plt.pause(0.001)  # pause a bit so that plots are updated"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def visualize_model(model, dataloaders, classes, num_images=6):\n",
    "    was_training = model.training\n",
    "    model.eval()\n",
    "    images_so_far = 0\n",
    "    fig = plt.figure()\n",
    "    with torch.no_grad():\n",
    "        for i, (inputs, labels) in enumerate(dataloaders['val']):\n",
    "            inputs = inputs.to(device)\n",
    "            labels = labels.to(device)\n",
    "\n",
    "            outputs = model(inputs)\n",
    "            _, preds = torch.max(outputs, 1)\n",
    "\n",
    "            for j in range(inputs.size()[0]):\n",
    "                images_so_far += 1\n",
    "                ax = plt.subplot(num_images//2, 2, images_so_far)\n",
    "                ax.axis('off')\n",
    "                ax.set_title(f'predicted: {classes[preds[j]]}')\n",
    "                imshow(inputs.cpu().data[j])\n",
    "                if images_so_far == num_images:\n",
    "                    model.train(mode=was_training)\n",
    "                return\n",
    "        model.train(mode=was_training)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2023-02-05 22:13:09,116 | INFO : Train dataset size: 35046\n",
      "2023-02-05 22:13:09,117 | INFO : Val dataset size: 8762\n"
     ]
    }
   ],
   "source": [
    "dataloaders = {\"train\": train_loader, \"val\": val_loader}\n",
    "train_dataset_size = len(train_loader.dataset)\n",
    "val_dataset_size = len(val_loader.dataset)\n",
    "dataset_sizes = {\"train\": train_dataset_size, \"val\": val_dataset_size}\n",
    "logging.info(f\"Train dataset size: {train_dataset_size}\")\n",
    "logging.info(f\"Val dataset size: {val_dataset_size}\")\n",
    "# Get the latest version of the Resnet weights and freeze the layers\n",
    "model_conv = torchvision.models.resnet18(weights=ResNet18_Weights.DEFAULT)\n",
    "for param in model_conv.parameters():\n",
    "    param.requires_grad = False\n",
    "# Parameters of newly constructed modules have requires_grad=True by default\n",
    "num_ftrs = model_conv.fc.in_features\n",
    "model_conv.fc = nn.Linear(num_ftrs, 2)\n",
    "model_conv = model_conv.to(device)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "# Only parameters of final layer are being optimized\n",
    "optimizer_ft = optim.Adam(model_conv.fc.parameters(), lr=0.0001)\n",
    "# Decay LR by a factor of 0.1 every 7 epochs\n",
    "exp_lr_scheduler = lr_scheduler.StepLR(optimizer_ft, step_size=7, gamma=0.1)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Set up Checkpoint and Early Stopping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "if os.path.exists(gl.chk_dir) == False:\n",
    "    os.mkdir(gl.chk_dir)\n",
    "    \n",
    "checkpoint_path = os.path.join(gl.chk_dir, gl.chk_resnet_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_checkpoint(checkpoint_path, model_conv, optimizer_ft):\n",
    "    checkpoint = torch.load(checkpoint_path)\n",
    "    model_conv.load_state_dict(checkpoint[\"model_state_dict\"])\n",
    "    optimizer_ft.load_state_dict(checkpoint[\"model_state_dict\"])\n",
    "    epoch = checkpoint[\"epoch\"]\n",
    "    loss = checkpoint[\"loss\"]\n",
    "    return checkpoint, model_conv, optimizer_ft, epoch, loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2023-02-05 22:13:11,269 | INFO : Epoch 1/25\n",
      "2023-02-05 22:13:11,270 | INFO : ----------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\Anaconda\\lib\\site-packages\\PIL\\Image.py:945: UserWarning: Palette images with Transparency expressed in bytes should be converted to RGBA images\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2023-02-05 23:05:54,140 | INFO : train Loss: 0.5701 Acc: 0.7021\n",
      "2023-02-05 23:16:38,742 | INFO : val Loss: 0.5113 Acc: 0.7414\n",
      "Best validation accuracy: 0.7413832458342844\n",
      "Saving best model for epoch: 1\n",
      "\n",
      "2023-02-05 23:16:40,305 | INFO : Epoch 2/25\n",
      "2023-02-05 23:16:40,306 | INFO : ----------\n",
      "2023-02-05 23:51:46,236 | INFO : train Loss: 0.5110 Acc: 0.7493\n",
      "2023-02-06 00:00:41,381 | INFO : val Loss: 0.4893 Acc: 0.7659\n",
      "Best validation accuracy: 0.7659210225975804\n",
      "Saving best model for epoch: 2\n",
      "\n",
      "2023-02-06 00:00:41,680 | INFO : Epoch 3/25\n",
      "2023-02-06 00:00:41,681 | INFO : ----------\n",
      "2023-02-06 00:44:51,571 | INFO : train Loss: 0.4921 Acc: 0.7652\n",
      "2023-02-06 00:55:09,686 | INFO : val Loss: 0.4734 Acc: 0.7868\n",
      "Best validation accuracy: 0.7868066651449441\n",
      "Saving best model for epoch: 3\n",
      "\n",
      "2023-02-06 00:55:12,343 | INFO : Epoch 4/25\n",
      "2023-02-06 00:55:12,344 | INFO : ----------\n",
      "2023-02-06 01:54:08,208 | INFO : train Loss: 0.4869 Acc: 0.7683\n",
      "2023-02-06 02:06:05,435 | INFO : val Loss: 0.4772 Acc: 0.7707\n",
      "EarlyStopping counter: 1 out of 2\n",
      "2023-02-06 02:06:05,446 | INFO : Early stopping for epoch: 4\n",
      "\n",
      "2023-02-06 02:06:05,554 | INFO : Epoch 5/25\n",
      "2023-02-06 02:06:05,555 | INFO : ----------\n",
      "2023-02-06 03:11:27,743 | INFO : train Loss: 0.4826 Acc: 0.7710\n",
      "2023-02-06 03:22:08,733 | INFO : val Loss: 0.4644 Acc: 0.7822\n",
      "2023-02-06 03:22:08,741 | INFO : Early stopping for epoch: 5\n",
      "\n",
      "2023-02-06 03:22:08,742 | INFO : Epoch 6/25\n",
      "2023-02-06 03:22:08,743 | INFO : ----------\n",
      "2023-02-06 04:07:26,351 | INFO : train Loss: 0.4808 Acc: 0.7739\n",
      "2023-02-06 04:20:50,552 | INFO : val Loss: 0.4702 Acc: 0.7824\n",
      "EarlyStopping counter: 1 out of 2\n",
      "2023-02-06 04:20:50,563 | INFO : Early stopping for epoch: 6\n",
      "\n",
      "2023-02-06 04:20:50,564 | INFO : Epoch 7/25\n",
      "2023-02-06 04:20:50,565 | INFO : ----------\n",
      "2023-02-06 05:14:57,911 | INFO : train Loss: 0.4757 Acc: 0.7734\n",
      "2023-02-06 05:25:01,326 | INFO : val Loss: 0.4637 Acc: 0.7795\n",
      "2023-02-06 05:25:01,335 | INFO : Early stopping for epoch: 7\n",
      "\n",
      "2023-02-06 05:25:01,356 | INFO : Epoch 8/25\n",
      "2023-02-06 05:25:01,356 | INFO : ----------\n",
      "2023-02-06 06:10:26,193 | INFO : train Loss: 0.4674 Acc: 0.7797\n",
      "2023-02-06 06:21:50,231 | INFO : val Loss: 0.4584 Acc: 0.7822\n",
      "2023-02-06 06:21:50,242 | INFO : Early stopping for epoch: 8\n",
      "\n",
      "2023-02-06 06:21:50,243 | INFO : Epoch 9/25\n",
      "2023-02-06 06:21:50,244 | INFO : ----------\n",
      "2023-02-06 07:11:47,649 | INFO : train Loss: 0.4691 Acc: 0.7805\n",
      "2023-02-06 07:23:57,093 | INFO : val Loss: 0.4661 Acc: 0.7796\n",
      "EarlyStopping counter: 1 out of 2\n",
      "2023-02-06 07:23:57,102 | INFO : Early stopping for epoch: 9\n",
      "\n",
      "2023-02-06 07:23:57,103 | INFO : Epoch 10/25\n",
      "2023-02-06 07:23:57,103 | INFO : ----------\n",
      "2023-02-06 08:02:32,925 | INFO : train Loss: 0.4707 Acc: 0.7780\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32md:\\mma\\Task6\\MagnimindTask6\\S4_TwitterProject.ipynb Cell 9\u001b[0m in \u001b[0;36m<cell line: 7>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      <a href='vscode-notebook-cell:/d%3A/mma/Task6/MagnimindTask6/S4_TwitterProject.ipynb#X11sZmlsZQ%3D%3D?line=2'>3</a>\u001b[0m     checkpoint, model_conv, optimizer_ft, epoch, loss \u001b[39m=\u001b[39m \\\n\u001b[0;32m      <a href='vscode-notebook-cell:/d%3A/mma/Task6/MagnimindTask6/S4_TwitterProject.ipynb#X11sZmlsZQ%3D%3D?line=3'>4</a>\u001b[0m         load_checkpoint(checkpoint_path, model_conv, optimizer_ft)\n\u001b[0;32m      <a href='vscode-notebook-cell:/d%3A/mma/Task6/MagnimindTask6/S4_TwitterProject.ipynb#X11sZmlsZQ%3D%3D?line=5'>6</a>\u001b[0m patience \u001b[39m=\u001b[39m \u001b[39m2\u001b[39m\n\u001b[1;32m----> <a href='vscode-notebook-cell:/d%3A/mma/Task6/MagnimindTask6/S4_TwitterProject.ipynb#X11sZmlsZQ%3D%3D?line=6'>7</a>\u001b[0m model_conv \u001b[39m=\u001b[39m train_model(model_conv, logging, criterion, optimizer_ft, exp_lr_scheduler, dataloaders, dataset_sizes, \n\u001b[0;32m      <a href='vscode-notebook-cell:/d%3A/mma/Task6/MagnimindTask6/S4_TwitterProject.ipynb#X11sZmlsZQ%3D%3D?line=7'>8</a>\u001b[0m                          checkpoint_path, patience, num_epochs\u001b[39m=\u001b[39;49m\u001b[39m25\u001b[39;49m)\n\u001b[0;32m     <a href='vscode-notebook-cell:/d%3A/mma/Task6/MagnimindTask6/S4_TwitterProject.ipynb#X11sZmlsZQ%3D%3D?line=9'>10</a>\u001b[0m save_pickle_object(model_conv, gl\u001b[39m.\u001b[39mpkl_model_conv)\n",
      "File \u001b[1;32md:\\mma\\Task6\\MagnimindTask6\\_model.py:27\u001b[0m, in \u001b[0;36mtrain_model\u001b[1;34m(model, logging, criterion, optimizer, scheduler, dataloaders, dataset_sizes, checkpoint_path, patience, num_epochs)\u001b[0m\n\u001b[0;32m     24\u001b[0m running_corrects \u001b[39m=\u001b[39m \u001b[39m0\u001b[39m\n\u001b[0;32m     26\u001b[0m \u001b[39m# Iterate over data.\u001b[39;00m\n\u001b[1;32m---> 27\u001b[0m \u001b[39mfor\u001b[39;00m inputs, labels \u001b[39min\u001b[39;00m dataloaders[phase]:\n\u001b[0;32m     28\u001b[0m     inputs \u001b[39m=\u001b[39m inputs\u001b[39m.\u001b[39mto(device)\n\u001b[0;32m     29\u001b[0m     labels \u001b[39m=\u001b[39m labels\u001b[39m.\u001b[39mto(device)\n",
      "File \u001b[1;32md:\\Anaconda\\lib\\site-packages\\torch\\utils\\data\\dataloader.py:628\u001b[0m, in \u001b[0;36m_BaseDataLoaderIter.__next__\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    625\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_sampler_iter \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m    626\u001b[0m     \u001b[39m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001b[39;00m\n\u001b[0;32m    627\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_reset()  \u001b[39m# type: ignore[call-arg]\u001b[39;00m\n\u001b[1;32m--> 628\u001b[0m data \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_next_data()\n\u001b[0;32m    629\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_num_yielded \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m \u001b[39m1\u001b[39m\n\u001b[0;32m    630\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_dataset_kind \u001b[39m==\u001b[39m _DatasetKind\u001b[39m.\u001b[39mIterable \u001b[39mand\u001b[39;00m \\\n\u001b[0;32m    631\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_IterableDataset_len_called \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39mand\u001b[39;00m \\\n\u001b[0;32m    632\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_num_yielded \u001b[39m>\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_IterableDataset_len_called:\n",
      "File \u001b[1;32md:\\Anaconda\\lib\\site-packages\\torch\\utils\\data\\dataloader.py:671\u001b[0m, in \u001b[0;36m_SingleProcessDataLoaderIter._next_data\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    669\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_next_data\u001b[39m(\u001b[39mself\u001b[39m):\n\u001b[0;32m    670\u001b[0m     index \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_next_index()  \u001b[39m# may raise StopIteration\u001b[39;00m\n\u001b[1;32m--> 671\u001b[0m     data \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_dataset_fetcher\u001b[39m.\u001b[39;49mfetch(index)  \u001b[39m# may raise StopIteration\u001b[39;00m\n\u001b[0;32m    672\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_pin_memory:\n\u001b[0;32m    673\u001b[0m         data \u001b[39m=\u001b[39m _utils\u001b[39m.\u001b[39mpin_memory\u001b[39m.\u001b[39mpin_memory(data, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_pin_memory_device)\n",
      "File \u001b[1;32md:\\Anaconda\\lib\\site-packages\\torch\\utils\\data\\_utils\\fetch.py:58\u001b[0m, in \u001b[0;36m_MapDatasetFetcher.fetch\u001b[1;34m(self, possibly_batched_index)\u001b[0m\n\u001b[0;32m     56\u001b[0m         data \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdataset\u001b[39m.\u001b[39m__getitems__(possibly_batched_index)\n\u001b[0;32m     57\u001b[0m     \u001b[39melse\u001b[39;00m:\n\u001b[1;32m---> 58\u001b[0m         data \u001b[39m=\u001b[39m [\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdataset[idx] \u001b[39mfor\u001b[39;00m idx \u001b[39min\u001b[39;00m possibly_batched_index]\n\u001b[0;32m     59\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m     60\u001b[0m     data \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdataset[possibly_batched_index]\n",
      "File \u001b[1;32md:\\Anaconda\\lib\\site-packages\\torch\\utils\\data\\_utils\\fetch.py:58\u001b[0m, in \u001b[0;36m<listcomp>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m     56\u001b[0m         data \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdataset\u001b[39m.\u001b[39m__getitems__(possibly_batched_index)\n\u001b[0;32m     57\u001b[0m     \u001b[39melse\u001b[39;00m:\n\u001b[1;32m---> 58\u001b[0m         data \u001b[39m=\u001b[39m [\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mdataset[idx] \u001b[39mfor\u001b[39;00m idx \u001b[39min\u001b[39;00m possibly_batched_index]\n\u001b[0;32m     59\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m     60\u001b[0m     data \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdataset[possibly_batched_index]\n",
      "File \u001b[1;32md:\\Anaconda\\lib\\site-packages\\torch\\utils\\data\\dataset.py:295\u001b[0m, in \u001b[0;36mSubset.__getitem__\u001b[1;34m(self, idx)\u001b[0m\n\u001b[0;32m    293\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39misinstance\u001b[39m(idx, \u001b[39mlist\u001b[39m):\n\u001b[0;32m    294\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdataset[[\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mindices[i] \u001b[39mfor\u001b[39;00m i \u001b[39min\u001b[39;00m idx]]\n\u001b[1;32m--> 295\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mdataset[\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mindices[idx]]\n",
      "File \u001b[1;32md:\\Anaconda\\lib\\site-packages\\torchvision\\datasets\\folder.py:229\u001b[0m, in \u001b[0;36mDatasetFolder.__getitem__\u001b[1;34m(self, index)\u001b[0m\n\u001b[0;32m    221\u001b[0m \u001b[39m\"\"\"\u001b[39;00m\n\u001b[0;32m    222\u001b[0m \u001b[39mArgs:\u001b[39;00m\n\u001b[0;32m    223\u001b[0m \u001b[39m    index (int): Index\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    226\u001b[0m \u001b[39m    tuple: (sample, target) where target is class_index of the target class.\u001b[39;00m\n\u001b[0;32m    227\u001b[0m \u001b[39m\"\"\"\u001b[39;00m\n\u001b[0;32m    228\u001b[0m path, target \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39msamples[index]\n\u001b[1;32m--> 229\u001b[0m sample \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mloader(path)\n\u001b[0;32m    230\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtransform \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m    231\u001b[0m     sample \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtransform(sample)\n",
      "File \u001b[1;32md:\\Anaconda\\lib\\site-packages\\torchvision\\datasets\\folder.py:268\u001b[0m, in \u001b[0;36mdefault_loader\u001b[1;34m(path)\u001b[0m\n\u001b[0;32m    266\u001b[0m     \u001b[39mreturn\u001b[39;00m accimage_loader(path)\n\u001b[0;32m    267\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m--> 268\u001b[0m     \u001b[39mreturn\u001b[39;00m pil_loader(path)\n",
      "File \u001b[1;32md:\\Anaconda\\lib\\site-packages\\torchvision\\datasets\\folder.py:248\u001b[0m, in \u001b[0;36mpil_loader\u001b[1;34m(path)\u001b[0m\n\u001b[0;32m    246\u001b[0m \u001b[39mwith\u001b[39;00m \u001b[39mopen\u001b[39m(path, \u001b[39m\"\u001b[39m\u001b[39mrb\u001b[39m\u001b[39m\"\u001b[39m) \u001b[39mas\u001b[39;00m f:\n\u001b[0;32m    247\u001b[0m     img \u001b[39m=\u001b[39m Image\u001b[39m.\u001b[39mopen(f)\n\u001b[1;32m--> 248\u001b[0m     \u001b[39mreturn\u001b[39;00m img\u001b[39m.\u001b[39;49mconvert(\u001b[39m\"\u001b[39;49m\u001b[39mRGB\u001b[39;49m\u001b[39m\"\u001b[39;49m)\n",
      "File \u001b[1;32md:\\Anaconda\\lib\\site-packages\\PIL\\Image.py:889\u001b[0m, in \u001b[0;36mImage.convert\u001b[1;34m(self, mode, matrix, dither, palette, colors)\u001b[0m\n\u001b[0;32m    847\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mconvert\u001b[39m(\u001b[39mself\u001b[39m, mode\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m, matrix\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m, dither\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m, palette\u001b[39m=\u001b[39mWEB, colors\u001b[39m=\u001b[39m\u001b[39m256\u001b[39m):\n\u001b[0;32m    848\u001b[0m     \u001b[39m\"\"\"\u001b[39;00m\n\u001b[0;32m    849\u001b[0m \u001b[39m    Returns a converted copy of this image. For the \"P\" mode, this\u001b[39;00m\n\u001b[0;32m    850\u001b[0m \u001b[39m    method translates pixels through the palette.  If mode is\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    886\u001b[0m \u001b[39m    :returns: An :py:class:`~PIL.Image.Image` object.\u001b[39;00m\n\u001b[0;32m    887\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[1;32m--> 889\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mload()\n\u001b[0;32m    891\u001b[0m     has_transparency \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39minfo\u001b[39m.\u001b[39mget(\u001b[39m\"\u001b[39m\u001b[39mtransparency\u001b[39m\u001b[39m\"\u001b[39m) \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m\n\u001b[0;32m    892\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m mode \u001b[39mand\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mmode \u001b[39m==\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mP\u001b[39m\u001b[39m\"\u001b[39m:\n\u001b[0;32m    893\u001b[0m         \u001b[39m# determine default mode\u001b[39;00m\n",
      "File \u001b[1;32md:\\Anaconda\\lib\\site-packages\\PIL\\ImageFile.py:235\u001b[0m, in \u001b[0;36mImageFile.load\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    233\u001b[0m \u001b[39mwhile\u001b[39;00m \u001b[39mTrue\u001b[39;00m:\n\u001b[0;32m    234\u001b[0m     \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m--> 235\u001b[0m         s \u001b[39m=\u001b[39m read(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mdecodermaxblock)\n\u001b[0;32m    236\u001b[0m     \u001b[39mexcept\u001b[39;00m (\u001b[39mIndexError\u001b[39;00m, struct\u001b[39m.\u001b[39merror) \u001b[39mas\u001b[39;00m e:\n\u001b[0;32m    237\u001b[0m         \u001b[39m# truncated png/gif\u001b[39;00m\n\u001b[0;32m    238\u001b[0m         \u001b[39mif\u001b[39;00m LOAD_TRUNCATED_IMAGES:\n",
      "File \u001b[1;32md:\\Anaconda\\lib\\site-packages\\PIL\\JpegImagePlugin.py:402\u001b[0m, in \u001b[0;36mJpegImageFile.load_read\u001b[1;34m(self, read_bytes)\u001b[0m\n\u001b[0;32m    396\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mload_read\u001b[39m(\u001b[39mself\u001b[39m, read_bytes):\n\u001b[0;32m    397\u001b[0m     \u001b[39m\"\"\"\u001b[39;00m\n\u001b[0;32m    398\u001b[0m \u001b[39m    internal: read more image data\u001b[39;00m\n\u001b[0;32m    399\u001b[0m \u001b[39m    For premature EOF and LOAD_TRUNCATED_IMAGES adds EOI marker\u001b[39;00m\n\u001b[0;32m    400\u001b[0m \u001b[39m    so libjpeg can finish decoding\u001b[39;00m\n\u001b[0;32m    401\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[1;32m--> 402\u001b[0m     s \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mfp\u001b[39m.\u001b[39;49mread(read_bytes)\n\u001b[0;32m    404\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m s \u001b[39mand\u001b[39;00m ImageFile\u001b[39m.\u001b[39mLOAD_TRUNCATED_IMAGES \u001b[39mand\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mhasattr\u001b[39m(\u001b[39mself\u001b[39m, \u001b[39m\"\u001b[39m\u001b[39m_ended\u001b[39m\u001b[39m\"\u001b[39m):\n\u001b[0;32m    405\u001b[0m         \u001b[39m# Premature EOF.\u001b[39;00m\n\u001b[0;32m    406\u001b[0m         \u001b[39m# Pretend file is finished adding EOI marker\u001b[39;00m\n\u001b[0;32m    407\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_ended \u001b[39m=\u001b[39m \u001b[39mTrue\u001b[39;00m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mFailed to interrupt the Kernel. \n",
      "\u001b[1;31mUnable to start Kernel 'base (Python 3.9.12)' due to connection timeout. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "load_checkpoint_flag = False\n",
    "if load_checkpoint_flag:\n",
    "    checkpoint, model_conv, optimizer_ft, epoch, loss = \\\n",
    "        load_checkpoint(checkpoint_path, model_conv, optimizer_ft)\n",
    "    \n",
    "patience = 2\n",
    "model_conv = train_model(model_conv, logging, criterion, optimizer_ft, exp_lr_scheduler, dataloaders, dataset_sizes, \n",
    "                         checkpoint_path, patience, num_epochs=25)\n",
    "\n",
    "save_pickle_object(model_conv, gl.pkl_model_conv)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mFailed to start the Kernel. \n",
      "\u001b[1;31mUnable to start Kernel 'base (Python 3.9.12)' due to connection timeout. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "visualize_model(model_conv, dataloaders, classes)\n",
    "\n",
    "plt.ioff()\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "88279d2366fe020547cde40dd65aa0e3aa662a6ec1f3ca12d88834876c85e1a6"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
