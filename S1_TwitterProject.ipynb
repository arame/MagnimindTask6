{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "91039f7c-4705-45ef-93f9-c84bcfcde8f7",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Deep Learning Project\n",
    "\n",
    "You have developed a model in your NLP session to predict the topic of tweets by examining the text content of the postings. In this project, you will create a model to classify the topic of pictures used in the tweets to help predict the topic of the tweets later. Your model from the NLP session and this one later can be used to predict a tweet's topic by examining both textual and the visual content of the postings.\n",
    "\n",
    "You will first execute Step 1 to pull the images from the corresponding URL address of each image contained in the tweets. These images are already labeled manually by human editors in terms of whether the images belong to `Business` topic or not. The label `1` means the image belongs to the business topic and `2` means otherwise.\n",
    "\n",
    "In Step 2, you will create a classification algorithm. Please divide the dataset into train and test datasets. You may use your train dataset to validate the accuracy of your model when tuning up the hyperparametrs of your model. After finalizing training your model, test it on your test dataset and report its accuracy. You may use different accuracy metrics. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7fd5ea30-2ce8-42bd-8139-d8b6ee492fca",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Step 1: Download the images\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "550425a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import logging, os, requests, shutil\n",
    "from _logging import set_logging\n",
    "from _pckle import save_pickle_object, load_pickle_object\n",
    "from _utility import gl, get_perc\n",
    "\n",
    "set_logging(logging)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "4a21af23",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_photo_urls():\n",
    "    df_photo_urls = load_pickle_object(gl.pkl_df_photo_urls)\n",
    "    if df_photo_urls is not None:\n",
    "        logging.info(\"Topic assigned Photo Urls retrieved from storage\")\n",
    "        return df_photo_urls\n",
    "\n",
    "    logging.info(\"Topic assigned Photo Urls not currently stored\")\n",
    "    filepath = os.path.join(\"Files\", \"tweet_data.csv\")\n",
    "    logging.info(f\"Read data from {filepath}\")\n",
    "    df_tweets_all = pd.read_csv(filepath)\n",
    "    # remove tweets with no photo urls\n",
    "    df_tweets = df_tweets_all[~df_tweets_all[gl.photoUrl].isnull()]\n",
    "    num_of_rows = len(df_tweets_all)\n",
    "    logging.info(f\"There are {num_of_rows} entries\")\n",
    "    df_tweets_all.drop_duplicates(subset=gl.photoUrl, inplace=True)\n",
    "    num_of_rows_after = len(df_tweets_all)\n",
    "    logging.info(f\"Number of duplicate photo rows deleted = {num_of_rows - num_of_rows_after}\")\n",
    "    # select only relevant columns\n",
    "    df_photo_urls = df_tweets[[gl.photoUrl, gl.topic]].copy()\n",
    "    # we are interested if the topic is or is not Business, so add a flag column\n",
    "    df_topic = pd.get_dummies(df_photo_urls[gl.topic], columns=[gl.topic])\n",
    "    df_photo_urls[gl.is_business] = df_topic[gl.business]\n",
    "    save_pickle_object(df_photo_urls, gl.pkl_df_photo_urls)\n",
    "    return df_photo_urls\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e947520a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2023-02-01 05:52:05,355 | INFO : Pickle file in: pickle\\pkl_df_photo_urls.pkl\n",
      "2023-02-01 05:52:05,356 | INFO : Topic assigned Photo Urls not currently stored\n",
      "2023-02-01 05:52:05,357 | INFO : Read data from Files\\tweet_data.csv\n",
      "2023-02-01 05:52:09,599 | INFO : There are 785916 entries\n",
      "2023-02-01 05:52:10,090 | INFO : Number of duplicate photo rows deleted = 530830\n",
      "2023-02-01 05:52:10,355 | INFO : Saving pickle file from: pickle\\pkl_df_photo_urls.pkl\n",
      "2023-02-01 05:52:10,567 | INFO : There are 277896 photos, of which 20134 (7.25%) are of Business\n"
     ]
    }
   ],
   "source": [
    "df_photo_urls = get_photo_urls()\n",
    "total_urls = len(df_photo_urls)\n",
    "total_is_business = sum(df_photo_urls[gl.is_business])\n",
    "perc_business = get_perc(total_is_business, total_urls)\n",
    "logging.info(f\"There are {total_urls} photos, of which {total_is_business} ({perc_business}%) are of Business\")\n",
    "\n",
    "#df_photo_urls.head(15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "b6b5f57b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_folder(folder):\n",
    "    if os.path.exists(folder) == False:\n",
    "        os.makedirs(folder)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "804e3ed4",
   "metadata": {},
   "source": [
    "Some image urls may no longer exist. So the best option is to download them to a local folder."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "9ad9eaaa",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_url_file_path(row, i, business_folder, other_folder):\n",
    "    url = row[0]\n",
    "    url_parts = url.split(\".\")\n",
    "    index = len(url_parts) - 1\n",
    "    ext = url_parts[index]\n",
    "    file_name = f\"{i}.{ext}\"\n",
    "    folder = business_folder if row[1] == \"Business\" else other_folder\n",
    "    file_path = os.path.join(folder, file_name)\n",
    "    return url, file_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "a0508462",
   "metadata": {},
   "outputs": [],
   "source": [
    "def download_images(df_photo_urls):\n",
    "    business_folder = os.path.join(\"Images\", \"Business\")\n",
    "    if os.path.exists(business_folder):\n",
    "        logging.info(\"Images already copied across\")\n",
    "        return\n",
    "        \n",
    "    other_folder = os.path.join(\"Images\", \"Other\")\n",
    "    create_folder(business_folder)\n",
    "    create_folder(other_folder)\n",
    "    np_photo_urls = df_photo_urls.to_numpy()\n",
    "    invalid_url_cnt = 0\n",
    "    exception_cnt = 0\n",
    "    successful_cnt = 0\n",
    "    logging.info(\"------- Start copying the images\")\n",
    "    for i, row in enumerate(np_photo_urls, start=1):\n",
    "        if i % 1000 == 0:\n",
    "            logging.info(f\"----Copying the {i} image\")\n",
    "        url, file_path = get_url_file_path(row, i, business_folder, other_folder)\n",
    "        try:\n",
    "            res = requests.get(url, stream = True)\n",
    "            if res.status_code == 200:\n",
    "                with open(file_path,'wb') as f:\n",
    "                    shutil.copyfileobj(res.raw, f)\n",
    "                successful_cnt += 1\n",
    "            else:\n",
    "                invalid_url_cnt += 1\n",
    "                #logging.info(f\"!! {i} Image from url {url} cannot be retreived\")\n",
    "        except:\n",
    "            exception_cnt += 1\n",
    "            logging.info(f\"!! {i} EXCEPTION: Image from url {url} cannot be retreived\")\n",
    "            \n",
    "    logging.info(\"** All Images still available downloaded\")\n",
    "    logging.info(f\"{successful_cnt} ({get_perc(successful_cnt, i)}%) images were successfully downloaded\")\n",
    "    logging.info(f\"{invalid_url_cnt} ({get_perc(invalid_url_cnt, i)}%) image urls are invalid\")\n",
    "    logging.info(f\"{exception_cnt} ({get_perc(exception_cnt, i)}%) attempted image downloads caused exceptions\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "78b4dfda",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2023-02-01 05:53:04,241 | INFO : ------- Start copying the images\n",
      "2023-02-01 05:54:47,495 | INFO : ----Copying the 1000 image\n",
      "2023-02-01 05:56:18,630 | INFO : ----Copying the 2000 image\n",
      "2023-02-01 05:59:03,021 | INFO : ----Copying the 3000 image\n",
      "2023-02-01 06:00:30,646 | INFO : ----Copying the 4000 image\n",
      "2023-02-01 06:01:56,006 | INFO : ----Copying the 5000 image\n",
      "2023-02-01 06:03:35,439 | INFO : ----Copying the 6000 image\n",
      "2023-02-01 06:05:09,023 | INFO : ----Copying the 7000 image\n",
      "2023-02-01 06:06:38,211 | INFO : ----Copying the 8000 image\n",
      "2023-02-01 06:08:05,667 | INFO : ----Copying the 9000 image\n",
      "2023-02-01 06:13:36,347 | INFO : ----Copying the 10000 image\n",
      "2023-02-01 06:17:14,128 | INFO : ----Copying the 11000 image\n",
      "2023-02-01 06:18:39,789 | INFO : ----Copying the 12000 image\n",
      "2023-02-01 06:22:22,278 | INFO : ----Copying the 13000 image\n",
      "2023-02-01 06:27:28,717 | INFO : ----Copying the 14000 image\n",
      "2023-02-01 06:31:26,233 | INFO : ----Copying the 15000 image\n",
      "2023-02-01 06:33:39,419 | INFO : ----Copying the 16000 image\n",
      "2023-02-01 06:37:25,853 | INFO : ----Copying the 17000 image\n",
      "2023-02-01 06:41:11,444 | INFO : ----Copying the 18000 image\n",
      "2023-02-01 06:44:40,856 | INFO : ----Copying the 19000 image\n",
      "2023-02-01 06:53:39,285 | INFO : ----Copying the 20000 image\n",
      "2023-02-01 07:02:58,270 | INFO : ----Copying the 21000 image\n",
      "2023-02-01 07:11:51,168 | INFO : ----Copying the 22000 image\n",
      "2023-02-01 07:20:39,774 | INFO : ----Copying the 23000 image\n",
      "2023-02-01 07:28:26,046 | INFO : ----Copying the 24000 image\n",
      "2023-02-01 07:37:02,002 | INFO : ----Copying the 25000 image\n",
      "2023-02-01 07:45:40,783 | INFO : ----Copying the 26000 image\n",
      "2023-02-01 07:54:37,367 | INFO : ----Copying the 27000 image\n",
      "2023-02-01 08:03:21,950 | INFO : ----Copying the 28000 image\n",
      "2023-02-01 08:11:53,544 | INFO : ----Copying the 29000 image\n",
      "2023-02-01 08:20:30,935 | INFO : ----Copying the 30000 image\n",
      "2023-02-01 08:29:10,041 | INFO : ----Copying the 31000 image\n",
      "2023-02-01 08:37:21,285 | INFO : ----Copying the 32000 image\n",
      "2023-02-01 08:46:35,633 | INFO : ----Copying the 33000 image\n",
      "2023-02-01 08:54:59,723 | INFO : ----Copying the 34000 image\n",
      "2023-02-01 09:03:37,487 | INFO : ----Copying the 35000 image\n",
      "2023-02-01 09:12:31,551 | INFO : ----Copying the 36000 image\n",
      "2023-02-01 09:21:01,917 | INFO : ----Copying the 37000 image\n",
      "2023-02-01 09:29:50,010 | INFO : ----Copying the 38000 image\n",
      "2023-02-01 09:38:43,168 | INFO : ----Copying the 39000 image\n",
      "2023-02-01 09:47:42,440 | INFO : ----Copying the 40000 image\n",
      "2023-02-01 09:56:07,661 | INFO : ----Copying the 41000 image\n",
      "2023-02-01 10:04:45,019 | INFO : ----Copying the 42000 image\n",
      "2023-02-01 10:14:29,359 | INFO : ----Copying the 43000 image\n",
      "2023-02-01 10:23:18,593 | INFO : ----Copying the 44000 image\n",
      "2023-02-01 10:31:57,294 | INFO : ----Copying the 45000 image\n",
      "2023-02-01 10:40:19,963 | INFO : ----Copying the 46000 image\n",
      "2023-02-01 10:48:45,348 | INFO : ----Copying the 47000 image\n",
      "2023-02-01 10:58:03,201 | INFO : ----Copying the 48000 image\n",
      "2023-02-01 11:07:08,298 | INFO : ----Copying the 49000 image\n",
      "2023-02-01 11:16:40,368 | INFO : ----Copying the 50000 image\n",
      "2023-02-01 11:25:50,156 | INFO : ----Copying the 51000 image\n",
      "2023-02-01 11:34:40,759 | INFO : ----Copying the 52000 image\n",
      "2023-02-01 11:43:18,282 | INFO : ----Copying the 53000 image\n",
      "2023-02-01 11:52:04,581 | INFO : ----Copying the 54000 image\n",
      "2023-02-01 12:00:57,143 | INFO : ----Copying the 55000 image\n",
      "2023-02-01 12:10:07,816 | INFO : ----Copying the 56000 image\n",
      "2023-02-01 12:19:28,938 | INFO : ----Copying the 57000 image\n"
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mFailed to interrupt the Kernel. \n",
      "\u001b[1;31mUnable to start Kernel 'base (Python 3.9.12)' due to connection timeout. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "download_images(df_photo_urls)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "d2260d87",
   "metadata": {},
   "source": [
    "Had to interupt the download process as the program appeared to stop for no apparent reason. We have enough images to train the data and create a classifier model. This will be done in the code ipynb file; P2_TwitterProject.ipynb"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12 (main, Apr  4 2022, 05:22:27) [MSC v.1916 64 bit (AMD64)]"
  },
  "vscode": {
   "interpreter": {
    "hash": "88279d2366fe020547cde40dd65aa0e3aa662a6ec1f3ca12d88834876c85e1a6"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
