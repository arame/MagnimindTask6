{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "91039f7c-4705-45ef-93f9-c84bcfcde8f7",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Deep Learning Project\n",
    "\n",
    "You have developed a model in your NLP session to predict the topic of tweets by examining the text content of the postings. In this project, you will create a model to classify the topic of pictures used in the tweets to help predict the topic of the tweets later. Your model from the NLP session and this one later can be used to predict a tweet's topic by examining both textual and the visual content of the postings.\n",
    "\n",
    "You will first execute Step 1 to pull the images from the corresponding URL address of each image contained in the tweets. These images are already labeled manually by human editors in terms of whether the images belong to `Nature` topic or not. The label `1` means the image belongs to the nature topic and `2` means otherwise.\n",
    "\n",
    "In Step 2, you will create a classification algorithm. Please divide the dataset into train and test datasets. You may use your train dataset to validate the accuracy of your model when tuning up the hyperparametrs of your model. After finalizing training your model, test it on your test dataset and report its accuracy. You may use different accuracy metrics. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7fd5ea30-2ce8-42bd-8139-d8b6ee492fca",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Step 1: Download the images\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "550425a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import logging, os, requests, shutil\n",
    "from sklearn.model_selection import train_test_split\n",
    "from _logging import set_logging\n",
    "from _metrics import display_metrics\n",
    "from _pckle import save_pickle_object, load_pickle_object\n",
    "from _utility import gl, get_perc\n",
    "\n",
    "set_logging(logging)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4a21af23",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_photo_urls():\n",
    "    df_photo_urls = load_pickle_object(gl.pkl_df_photo_urls)\n",
    "    if df_photo_urls.empty == False:\n",
    "        logging.info(\"Topic assigned Photo Urls retrieved from storage\")\n",
    "        return df_photo_urls\n",
    "\n",
    "    logging.info(\"Topic assigned Photo Urls not currently stored\")\n",
    "    filepath = os.path.join(\"Files\", \"tweet_data.csv\")\n",
    "    logging.info(f\"Read data from {filepath}\")\n",
    "    df_tweets_all = pd.read_csv(filepath)\n",
    "    # remove tweets with no photo urls\n",
    "    df_tweets = df_tweets_all[~df_tweets_all[gl.photoUrl].isnull()]\n",
    "    # select only relevant columns\n",
    "    df_photo_urls = df_tweets[[gl.photoUrl, gl.topic]].copy()\n",
    "    # we are interested if the topic is or is not Nature, so add a flag column\n",
    "    df_topic = pd.get_dummies(df_photo_urls[gl.topic], columns=[gl.topic])\n",
    "    df_photo_urls[gl.is_nature] = df_topic[gl.nature]\n",
    "    save_pickle_object(df_photo_urls, gl.pkl_df_photo_urls)\n",
    "    total_urls = len(df_photo_urls)\n",
    "    total_is_nature = sum(df_photo_urls[gl.is_nature])\n",
    "    perc_nature = get_perc(total_is_nature, total_urls)\n",
    "    logging.info(f\"There are {total_urls} photos, of which {total_is_nature} ({perc_nature}%) are of Nature\")\n",
    "    return df_photo_urls\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e947520a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2023-01-31 15:45:26,617 | INFO : Loading pickle file from: pickle\\pkl_df_photo_urls.pkl\n",
      "2023-01-31 15:45:26,665 | INFO : Topic assigned Photo Urls retrieved from storage\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>photoUrl</th>\n",
       "      <th>topicName</th>\n",
       "      <th>IsNature</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>https://pbs.twimg.com/media/Dtx8SiIWkAImVsb.jpg</td>\n",
       "      <td>Business</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>https://pbs.twimg.com/media/Dtx8yTyW4AEciqP.jpg</td>\n",
       "      <td>Business</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>https://pbs.twimg.com/media/Dtx83JvX4AE48aw.jpg</td>\n",
       "      <td>Business</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>https://pbs.twimg.com/media/DtyCu-ZXgAUimt7.jpg</td>\n",
       "      <td>Business</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>https://pbs.twimg.com/media/DtyA7KSW4AA5deF.jpg</td>\n",
       "      <td>Business</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>https://pbs.twimg.com/media/DtyAKObXQAAPUSN.jpg</td>\n",
       "      <td>Business</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>40</th>\n",
       "      <td>https://pbs.twimg.com/media/Dts0d_vWkAARiHg.jpg</td>\n",
       "      <td>Animal</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>42</th>\n",
       "      <td>https://pbs.twimg.com/media/DtyDOo0U8AAEl_o.jpg</td>\n",
       "      <td>Animal</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>44</th>\n",
       "      <td>https://pbs.twimg.com/media/DtuIrMXWoAA5Wve.jpg</td>\n",
       "      <td>Nature</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>47</th>\n",
       "      <td>https://pbs.twimg.com/media/Dcnhi_FXcAAQPXk.jpg</td>\n",
       "      <td>Nature</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>48</th>\n",
       "      <td>https://pbs.twimg.com/media/DttX-ubUcAUGp4S.jpg</td>\n",
       "      <td>Travel</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49</th>\n",
       "      <td>https://pbs.twimg.com/media/DttMJZDUwAEe-4L.jpg</td>\n",
       "      <td>Travel</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>56</th>\n",
       "      <td>https://pbs.twimg.com/media/Dtusrd2V4AAS5cD.jpg</td>\n",
       "      <td>Interesting</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>58</th>\n",
       "      <td>https://pbs.twimg.com/media/Dtx5V7bW4AAA5CK.jpg</td>\n",
       "      <td>Memes</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>71</th>\n",
       "      <td>https://pbs.twimg.com/media/DtyIco-XcAA02Qp.jpg</td>\n",
       "      <td>Business</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                           photoUrl    topicName  IsNature\n",
       "0   https://pbs.twimg.com/media/Dtx8SiIWkAImVsb.jpg     Business         0\n",
       "1   https://pbs.twimg.com/media/Dtx8yTyW4AEciqP.jpg     Business         0\n",
       "6   https://pbs.twimg.com/media/Dtx83JvX4AE48aw.jpg     Business         0\n",
       "15  https://pbs.twimg.com/media/DtyCu-ZXgAUimt7.jpg     Business         0\n",
       "21  https://pbs.twimg.com/media/DtyA7KSW4AA5deF.jpg     Business         0\n",
       "28  https://pbs.twimg.com/media/DtyAKObXQAAPUSN.jpg     Business         0\n",
       "40  https://pbs.twimg.com/media/Dts0d_vWkAARiHg.jpg       Animal         0\n",
       "42  https://pbs.twimg.com/media/DtyDOo0U8AAEl_o.jpg       Animal         0\n",
       "44  https://pbs.twimg.com/media/DtuIrMXWoAA5Wve.jpg       Nature         1\n",
       "47  https://pbs.twimg.com/media/Dcnhi_FXcAAQPXk.jpg       Nature         1\n",
       "48  https://pbs.twimg.com/media/DttX-ubUcAUGp4S.jpg       Travel         0\n",
       "49  https://pbs.twimg.com/media/DttMJZDUwAEe-4L.jpg       Travel         0\n",
       "56  https://pbs.twimg.com/media/Dtusrd2V4AAS5cD.jpg  Interesting         0\n",
       "58  https://pbs.twimg.com/media/Dtx5V7bW4AAA5CK.jpg        Memes         0\n",
       "71  https://pbs.twimg.com/media/DtyIco-XcAA02Qp.jpg     Business         0"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_photo_urls = get_photo_urls()\n",
    "df_photo_urls.head(15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b6b5f57b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_folder(folder):\n",
    "    if os.path.exists(folder) == False:\n",
    "        os.makedirs(folder)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "804e3ed4",
   "metadata": {},
   "source": [
    "Some image urls may no longer exist. So the best option is to download them to a local folder."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "9ad9eaaa",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_url_file_path(row, i, nature_folder, other_folder):\n",
    "    url = row[0]\n",
    "    url_parts = url.split(\".\")\n",
    "    index = len(url_parts) - 1\n",
    "    ext = url_parts[index]\n",
    "    file_name = f\"{i}.{ext}\"\n",
    "    folder = nature_folder if row[1] == \"Nature\" else other_folder\n",
    "    file_path = os.path.join(folder, file_name)\n",
    "    return url, file_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a0508462",
   "metadata": {},
   "outputs": [],
   "source": [
    "def download_images(df_photo_urls):\n",
    "    nature_folder = os.path.join(\"Images\", \"Nature\")\n",
    "    if os.path.exists(nature_folder):\n",
    "        logging.info(\"Images already copied across\")\n",
    "        return\n",
    "        \n",
    "    other_folder = os.path.join(\"Images\", \"Other\")\n",
    "    create_folder(nature_folder)\n",
    "    create_folder(other_folder)\n",
    "    np_photo_urls = df_photo_urls.to_numpy()\n",
    "    invalid_url_cnt = 0\n",
    "    exception_cnt = 0\n",
    "    successful_cnt = 0\n",
    "    logging.info(\"------- Start copying the images\")\n",
    "    for i, row in enumerate(np_photo_urls, start=1):\n",
    "        if i % 1000 == 0:\n",
    "            logging.info(f\"----Copying the {i} image\")\n",
    "        url, file_path = get_url_file_path(row, i, nature_folder, other_folder)\n",
    "        try:\n",
    "            res = requests.get(url, stream = True)\n",
    "            if res.status_code == 200:\n",
    "                with open(file_path,'wb') as f:\n",
    "                    shutil.copyfileobj(res.raw, f)\n",
    "                successful_cnt += 1\n",
    "            else:\n",
    "                invalid_url_cnt += 1\n",
    "                #logging.info(f\"!! {i} Image from url {url} cannot be retreived\")\n",
    "        except:\n",
    "            exception_cnt += 1\n",
    "            logging.info(f\"!! {i} EXCEPTION: Image from url {url} cannot be retreived\")\n",
    "\n",
    "    logging.info(\"** All Images still available downloaded\")\n",
    "    logging.info(f\"{successful_cnt} ({get_perc(successful_cnt, i)}%) images were successfully downloaded\")\n",
    "    logging.info(f\"{invalid_url_cnt} ({get_perc(invalid_url_cnt, i)}%) image urls are invalid\")\n",
    "    logging.info(f\"{exception_cnt} ({get_perc(exception_cnt, i)}%) attempted image downloads caused exceptions\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "78b4dfda",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2023-01-31 15:45:34,948 | INFO : ------- Start copying the images\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Traceback (most recent call last):\n",
      "  File \"_pydevd_bundle/pydevd_cython.pyx\", line 1078, in _pydevd_bundle.pydevd_cython.PyDBFrame.trace_dispatch\n",
      "  File \"_pydevd_bundle/pydevd_cython.pyx\", line 297, in _pydevd_bundle.pydevd_cython.PyDBFrame.do_wait_suspend\n",
      "  File \"c:\\Users\\hijik\\Anaconda3\\lib\\site-packages\\debugpy\\_vendored\\pydevd\\pydevd.py\", line 1976, in do_wait_suspend\n",
      "    keep_suspended = self._do_wait_suspend(thread, frame, event, arg, suspend_type, from_this_thread, frames_tracker)\n",
      "  File \"c:\\Users\\hijik\\Anaconda3\\lib\\site-packages\\debugpy\\_vendored\\pydevd\\pydevd.py\", line 2011, in _do_wait_suspend\n",
      "    time.sleep(0.01)\n",
      "KeyboardInterrupt\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2023-01-31 15:46:05,896 | INFO : !! 420 EXCEPTION: Image from url https://pbs.twimg.com/media/Dt2TVCsW0AEHKYe.jpg cannot be retreived\n"
     ]
    }
   ],
   "source": [
    "download_images(df_photo_urls)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "ada53740",
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_data(df_photo_urls):\n",
    "    X_temp, y_temp, X_test, y_test = train_test_split(df_photo_urls[gl.photoUrl], df_photo_urls[gl.is_nature], test_size=0.05, stratify=df_photo_urls[gl.is_nature])\n",
    "    X_train, y_train, X_val, y_val = train_test_split(X_temp, y_temp, test_size=0.20, stratify=y_temp)\n",
    "    return X_train, y_train, X_val, y_val, X_test, y_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "1d8c4e4b",
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Found input variables with inconsistent numbers of samples: [264001, 13895]",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_29904\\2430280823.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mX_train\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mX_val\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_val\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mX_test\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_test\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0msplit_data\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdf_photo_urls\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_29904\\1137901204.py\u001b[0m in \u001b[0;36msplit_data\u001b[1;34m(df_photo_urls)\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;32mdef\u001b[0m \u001b[0msplit_data\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdf_photo_urls\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      2\u001b[0m     \u001b[0mX_temp\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_temp\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mX_test\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_test\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtrain_test_split\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdf_photo_urls\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mgl\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mphotoUrl\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdf_photo_urls\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mgl\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mis_nature\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtest_size\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m0.05\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mstratify\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mdf_photo_urls\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mgl\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mis_nature\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 3\u001b[1;33m     \u001b[0mX_train\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mX_val\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_val\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtrain_test_split\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX_temp\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_temp\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtest_size\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m0.20\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mstratify\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0my_temp\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      4\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[0mX_train\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mX_val\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_val\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mX_test\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_test\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\hijik\\Anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_split.py\u001b[0m in \u001b[0;36mtrain_test_split\u001b[1;34m(test_size, train_size, random_state, shuffle, stratify, *arrays)\u001b[0m\n\u001b[0;32m   2415\u001b[0m         \u001b[1;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"At least one array required as input\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2416\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 2417\u001b[1;33m     \u001b[0marrays\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mindexable\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0marrays\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   2418\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2419\u001b[0m     \u001b[0mn_samples\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0m_num_samples\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0marrays\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\hijik\\Anaconda3\\lib\\site-packages\\sklearn\\utils\\validation.py\u001b[0m in \u001b[0;36mindexable\u001b[1;34m(*iterables)\u001b[0m\n\u001b[0;32m    376\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    377\u001b[0m     \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0m_make_indexable\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mX\u001b[0m \u001b[1;32min\u001b[0m \u001b[0miterables\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 378\u001b[1;33m     \u001b[0mcheck_consistent_length\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0mresult\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    379\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[0mresult\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    380\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\hijik\\Anaconda3\\lib\\site-packages\\sklearn\\utils\\validation.py\u001b[0m in \u001b[0;36mcheck_consistent_length\u001b[1;34m(*arrays)\u001b[0m\n\u001b[0;32m    330\u001b[0m     \u001b[0muniques\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0munique\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlengths\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    331\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0muniques\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m>\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 332\u001b[1;33m         raise ValueError(\n\u001b[0m\u001b[0;32m    333\u001b[0m             \u001b[1;34m\"Found input variables with inconsistent numbers of samples: %r\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    334\u001b[0m             \u001b[1;33m%\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0ml\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0ml\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mlengths\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mValueError\u001b[0m: Found input variables with inconsistent numbers of samples: [264001, 13895]"
     ]
    }
   ],
   "source": [
    "X_train, y_train, X_val, y_val, X_test, y_test = split_data(df_photo_urls)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "d2260d87",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "3124b18c-b780-4d90-bdc9-f8ffb2f3c637",
   "metadata": {},
   "source": [
    "## Step 2: Classifier\n",
    "\n",
    "Develop a classifier for two categories. Create the necessary folders for the test and train datasets. Either create your own model or tranfer a model and revise it. Make sure you incorporate regularization, callbacks, etc., and use data augmentation. Since images may not be so distinct with respect to their categories, you may not get the same kind of performance you had in your assignments."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "690cff5b-0a25-434a-9961-ed17ab7ea3ee",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "vscode": {
   "interpreter": {
    "hash": "b41ba045e83f0be7a0a86cbeef029bed6bb1f3047ea5aef815a52ba8b6ba543c"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
